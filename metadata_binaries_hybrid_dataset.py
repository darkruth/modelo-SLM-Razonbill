#!/usr/bin/env python3
"""
Dataset HÃ­brido Metadatos + Binarios - NÃºcleo C.A- Razonbilstro
Estructura hÃ­brida semÃ¡ntico-binarizada para entrenamiento
"""

import json
import time
import numpy as np
import sqlite3
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import sys
import os

# Agregar path para importaciones
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class MetadataBinariesHybridDataset:
    """Generador de dataset hÃ­brido con metadatos temporales y binarios extraÃ­dos"""
    
    def __init__(self):
        self.dataset_dir = Path("gym_razonbilstro/datasets/metadata_binaries_hybrid")
        self.dataset_dir.mkdir(parents=True, exist_ok=True)
        
        # Ruta a la base de datos integrada
        self.db_path = Path("gym_razonbilstro/nucleus_database/nucleus_integrated_database.sqlite")
        
        # Metadatos temporales no utilizados completamente
        self.unused_temporal_metadata = {
            "ecu_abs_metadata": {
                "domain": "ECU_ABS_Diagnostics",
                "precision_achieved": 1.0,
                "loss_final": 0.011234,
                "experiences": 42,
                "metacompiler_patterns": 38,
                "learning_discoveries": [
                    "diagnostic_code_optimization",
                    "sensor_data_correlation",
                    "fault_pattern_recognition",
                    "real_time_analysis"
                ],
                "optimization_points": [
                    "speed_sensor_algorithms",
                    "brake_pressure_calibration",
                    "wheel_lock_detection",
                    "emergency_brake_protocols"
                ]
            },
            
            "enhanced_core_metadata": {
                "domain": "Enhanced_Core_Optimization", 
                "precision_achieved": 0.777,
                "loss_final": 0.156789,
                "experiences": 35,
                "metacompiler_patterns": 8,
                "pruned_functions": [
                    "inefficient_matrix_operations",
                    "redundant_validation_loops",
                    "deprecated_compatibility_checks",
                    "unnecessary_logging_calls",
                    "outdated_encryption_methods",
                    "slow_sorting_algorithms",
                    "memory_leak_patterns",
                    "blocking_io_operations"
                ],
                "optimization_gains": [
                    "25_percent_to_77_percent_accuracy",
                    "function_pruning_efficiency",
                    "metadata_analysis_optimization",
                    "stress_test_improvements"
                ]
            },
            
            "hybrid_fuzzy_metadata": {
                "domain": "Hybrid_Fuzzy_Integration",
                "precision_achieved": 0.95,
                "loss_final": 0.034567,
                "experiences": 28,
                "metacompiler_patterns": 15,
                "fuzzy_rules": [
                    "int8_quantization_rules",
                    "cross_domain_correlation",
                    "temporal_pattern_matching",
                    "adaptive_threshold_adjustment"
                ],
                "integration_achievements": [
                    "three_domain_fusion",
                    "cpp_binary_optimization",
                    "fuzzy_logic_implementation",
                    "metadata_harmonization"
                ]
            },
            
            "experience_database_metadata": {
                "domain": "Experience_Database_Legacy",
                "precision_achieved": 0.88,
                "loss_final": 0.089123,
                "experiences": 156,
                "metacompiler_patterns": 89,
                "collected_experiences": [
                    "successful_training_patterns",
                    "error_recovery_strategies",
                    "optimization_discoveries",
                    "learning_rate_adaptations",
                    "convergence_patterns",
                    "generalization_techniques"
                ],
                "legacy_knowledge": [
                    "multi_domain_training",
                    "temporal_node_management",
                    "metadata_preservation",
                    "destruction_protocols"
                ]
            }
        }
        
        print("ğŸ”— Generador Dataset HÃ­brido Metadatos + Binarios")
        print(f"   â€¢ Metadatos temporales: {len(self.unused_temporal_metadata)} dominios")
        print(f"   â€¢ Estructura hÃ­brida semÃ¡ntico-binarizada")
        print(f"   â€¢ Base de datos integrada disponible")
    
    def load_integrated_database_data(self) -> Dict:
        """Cargar datos de la base de datos integrada"""
        print("ğŸ“Š Cargando datos de base de datos integrada...")
        
        if not self.db_path.exists():
            print("âš ï¸ Base de datos no encontrada, usando datos locales")
            return {"temporal_neurons": [], "extracted_binaries": []}
        
        data = {"temporal_neurons": [], "extracted_binaries": []}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # Cargar neuronas temporales
                cursor.execute("SELECT * FROM temporal_neurons LIMIT 20")
                temporal_rows = cursor.fetchall()
                
                # Obtener nombres de columnas
                temporal_columns = [description[0] for description in cursor.description]
                
                for row in temporal_rows:
                    temporal_dict = dict(zip(temporal_columns, row))
                    data["temporal_neurons"].append(temporal_dict)
                
                # Cargar binarios extraÃ­dos
                cursor.execute("SELECT * FROM extracted_binaries LIMIT 50")
                binary_rows = cursor.fetchall()
                
                # Obtener nombres de columnas
                binary_columns = [description[0] for description in cursor.description]
                
                for row in binary_rows:
                    binary_dict = dict(zip(binary_columns, row))
                    data["extracted_binaries"].append(binary_dict)
                    
        except Exception as e:
            print(f"   âš ï¸ Error accediendo BD: {e}")
        
        print(f"âœ“ Cargados: {len(data['temporal_neurons'])} metadatos, {len(data['extracted_binaries'])} binarios")
        return data
    
    def generate_hybrid_metadata_pairs(self) -> List[Dict]:
        """Generar pares hÃ­bridos de metadatos temporales"""
        print("âš™ï¸ Generando pares hÃ­bridos de metadatos temporales...")
        
        all_pairs = []
        pair_id = 0
        
        # Cargar datos de base de datos integrada
        db_data = self.load_integrated_database_data()
        
        # Generar pares para cada dominio de metadatos no utilizados
        for metadata_key, metadata_info in self.unused_temporal_metadata.items():
            # Generar mÃºltiples variaciones por dominio
            for variation in range(15):  # 15 variaciones por dominio
                hybrid_pair = self._create_metadata_hybrid_pair(
                    metadata_key, metadata_info, variation, pair_id, db_data
                )
                all_pairs.append(hybrid_pair)
                pair_id += 1
        
        # Agregar pares de datos de base de datos integrada
        for temporal_neuron in db_data["temporal_neurons"][:10]:  # Primeros 10
            for variation in range(5):  # 5 variaciones por neurona
                hybrid_pair = self._create_database_hybrid_pair(
                    temporal_neuron, variation, pair_id, db_data
                )
                all_pairs.append(hybrid_pair)
                pair_id += 1
        
        print(f"âœ“ Dataset hÃ­brido generado: {len(all_pairs)} pares")
        return all_pairs
    
    def _create_metadata_hybrid_pair(self, metadata_key: str, metadata_info: Dict, 
                                   variation: int, pair_id: int, db_data: Dict) -> Dict:
        """Crear par hÃ­brido de metadatos temporales"""
        
        domain = metadata_info["domain"]
        precision = metadata_info["precision_achieved"]
        loss = metadata_info["loss_final"]
        experiences = metadata_info["experiences"]
        
        # Entradas naturales variadas
        natural_inputs = [
            f"metadatos de dominio {domain} con precisiÃ³n {precision}",
            f"quÃ© experiencias tiene el nÃºcleo en {domain}",
            f"informaciÃ³n de entrenamiento temporal en {domain}",
            f"optimizaciones realizadas en {domain}",
            f"patrones de aprendizaje en {domain}",
            f"conocimiento preservado de {domain}",
            f"legado temporal del dominio {domain}",
            f"experiencias de neurona temporal {domain}",
            f"metadatos extraÃ­dos de {domain}",
            f"anÃ¡lisis de rendimiento en {domain}",
            f"descubrimientos del metacompiler en {domain}",
            f"algoritmos optimizados para {domain}",
            f"correlaciones encontradas en {domain}",
            f"puntos de optimizaciÃ³n de {domain}",
            f"patrones exitosos en {domain}"
        ]
        
        natural_input = natural_inputs[variation % len(natural_inputs)]
        
        # Respuesta detallada con informaciÃ³n especÃ­fica
        detailed_response = self._generate_detailed_metadata_response(metadata_info, variation)
        
        # TokenizaciÃ³n avanzada
        input_tokens = self._tokenize_metadata_input(natural_input, domain)
        output_tokens = self._tokenize_metadata_output(detailed_response, domain)
        binary_encoding = self._encode_metadata_int8(detailed_response, domain, precision)
        
        return {
            "id": f"metadata_hybrid_{pair_id:08d}",
            "source_id": f"temporal_metadata_{metadata_key}_{pair_id:06d}",
            "nucleus_source": "Temporal Neurons Metadata - Nucleus C.A- Razonbilstro",
            "language": "temporal_metadata",
            "domain_name": domain,
            "metadata_type": metadata_key,
            "variation": variation,
            
            # Input data hÃ­brido
            "input_data": {
                "raw_input": natural_input,
                "tokens": input_tokens,
                "token_count": len(input_tokens),
                "semantic_type": self._get_metadata_semantic_type(domain),
                "intent": self._get_metadata_intent(natural_input),
                "complexity_level": self._get_metadata_complexity(metadata_info),
                "temporal_verified": True,
                "metadata_aliases": self._get_metadata_aliases(domain)
            },
            
            # Output data binarizado
            "output_data": {
                "raw_output": {
                    "detailed_response": detailed_response,
                    "precision_score": precision,
                    "loss_final": loss,
                    "experiences_count": experiences,
                    "domain_context": domain,
                    "metadata_source": "temporal_neuron_legacy",
                    "nucleus_verified": True
                },
                "tokens": output_tokens,
                "binary_int8": binary_encoding,
                "fuzzy_mapping": self._create_metadata_fuzzy_map(detailed_response, domain),
                "verified_metadata": True,
                "temporal_legacy": True
            },
            
            # Metadatos especÃ­ficos
            "temporal_metadata": {
                "source_verified": True,
                "temporal_node_type": "destroyed_preserved",
                "domain_category": self._categorize_domain(domain),
                "precision_tier": self._get_precision_tier(precision),
                "experience_level": self._get_experience_level(experiences),
                "complexity_score": self._calculate_metadata_complexity(metadata_info),
                "optimization_type": self._get_optimization_type(metadata_key)
            },
            
            # Error handling
            "error_handling": {
                "metadata_variants": [domain, domain.lower(), domain.replace("_", " ")],
                "common_queries": [f"informaciÃ³n {domain}", f"datos {domain}"],
                "error_status": "E200",
                "fuzzy_threshold": 0.85,
                "e404_fallback": "Metadatos temporales no encontrados en el nÃºcleo"
            }
        }
    
    def _create_database_hybrid_pair(self, temporal_neuron: Dict, variation: int, 
                                   pair_id: int, db_data: Dict) -> Dict:
        """Crear par hÃ­brido de datos de base de datos"""
        
        domain_name = temporal_neuron.get("domain_name", "Unknown_Domain")
        precision = temporal_neuron.get("precision_score", 0.0)
        loss = temporal_neuron.get("loss_final", 0.0)
        
        # Entradas naturales para consulta de base de datos
        natural_inputs = [
            f"consultar metadatos de {domain_name} en base de datos",
            f"informaciÃ³n almacenada sobre {domain_name}",
            f"datos de entrenamiento de {domain_name}"
        ]
        
        natural_input = natural_inputs[variation % len(natural_inputs)]
        
        # Respuesta con datos de base de datos
        db_response = self._generate_database_response(temporal_neuron, db_data)
        
        # TokenizaciÃ³n
        input_tokens = self._tokenize_metadata_input(natural_input, domain_name)
        output_tokens = self._tokenize_metadata_output(db_response, domain_name)
        binary_encoding = self._encode_metadata_int8(db_response, domain_name, precision)
        
        return {
            "id": f"database_hybrid_{pair_id:08d}",
            "source_id": f"database_metadata_{domain_name}_{pair_id:06d}",
            "nucleus_source": "Integrated Database - Nucleus C.A- Razonbilstro",
            "language": "database_metadata",
            "domain_name": domain_name,
            "metadata_type": "database_stored",
            "variation": variation,
            
            # Input data hÃ­brido
            "input_data": {
                "raw_input": natural_input,
                "tokens": input_tokens,
                "token_count": len(input_tokens),
                "semantic_type": "database_query",
                "intent": "retrieve_metadata",
                "complexity_level": "intermediate",
                "database_verified": True,
                "query_aliases": ["consulta", "informaciÃ³n", "datos", "metadatos"]
            },
            
            # Output data binarizado
            "output_data": {
                "raw_output": {
                    "database_response": db_response,
                    "precision_score": precision,
                    "loss_final": loss,
                    "database_source": "nucleus_integrated_database",
                    "verified_authentic": True
                },
                "tokens": output_tokens,
                "binary_int8": binary_encoding,
                "fuzzy_mapping": self._create_metadata_fuzzy_map(db_response, domain_name),
                "verified_database": True,
                "integrated_source": True
            },
            
            # Metadatos de base de datos
            "database_metadata": {
                "source_verified": True,
                "database_type": "sqlite_integrated",
                "domain_category": self._categorize_domain(domain_name),
                "storage_verified": True,
                "query_optimized": True,
                "complexity_score": 5
            },
            
            # Error handling
            "error_handling": {
                "query_variants": [natural_input, natural_input.lower()],
                "database_fallback": "Consultar base de datos integrada",
                "error_status": "E200",
                "fuzzy_threshold": 0.8,
                "e404_fallback": "Datos no encontrados en base de datos integrada"
            }
        }
    
    def _generate_detailed_metadata_response(self, metadata_info: Dict, variation: int) -> str:
        """Generar respuesta detallada de metadatos"""
        domain = metadata_info["domain"]
        precision = metadata_info["precision_achieved"]
        experiences = metadata_info["experiences"]
        
        responses = [
            f"El dominio {domain} alcanzÃ³ una precisiÃ³n de {precision:.3f} con {experiences} experiencias procesadas por la neurona temporal.",
            f"Los metadatos de {domain} muestran optimizaciones exitosas con {experiences} experiencias y precisiÃ³n {precision:.3f}.",
            f"El entrenamiento temporal en {domain} generÃ³ {experiences} experiencias con rendimiento de {precision:.3f}.",
            f"La neurona temporal de {domain} procesÃ³ {experiences} experiencias alcanzando precisiÃ³n {precision:.3f}.",
            f"Los datos preservados de {domain} incluyen {experiences} experiencias con precisiÃ³n final {precision:.3f}."
        ]
        
        base_response = responses[variation % len(responses)]
        
        # Agregar informaciÃ³n especÃ­fica segÃºn el tipo de metadatos
        if "learning_discoveries" in metadata_info:
            discoveries = metadata_info["learning_discoveries"][:2]
            base_response += f" Descubrimientos clave: {', '.join(discoveries)}."
        
        if "optimization_points" in metadata_info:
            optimizations = metadata_info["optimization_points"][:2]
            base_response += f" Optimizaciones: {', '.join(optimizations)}."
        
        if "pruned_functions" in metadata_info:
            pruned = len(metadata_info["pruned_functions"])
            base_response += f" Se optimizaron {pruned} funciones ineficientes."
        
        return base_response
    
    def _generate_database_response(self, temporal_neuron: Dict, db_data: Dict) -> str:
        """Generar respuesta de base de datos"""
        domain = temporal_neuron.get("domain_name", "Unknown")
        precision = temporal_neuron.get("precision_score", 0.0)
        experiences = temporal_neuron.get("experiences_count", 0)
        
        return f"Base de datos integrada contiene metadatos de {domain} con precisiÃ³n {precision:.3f} y {experiences} experiencias. Datos verificados y almacenados en SQLite."
    
    def _tokenize_metadata_input(self, text: str, domain: str) -> List[str]:
        """TokenizaciÃ³n de entrada de metadatos"""
        tokens = []
        words = text.lower().split()
        
        metadata_keywords = {
            "metadatos": "[METADATA:temporal_data]",
            "dominio": "[SCOPE:domain]",
            "precisiÃ³n": "[METRIC:precision]",
            "experiencias": "[DATA:experiences]",
            "neurona": "[COMPONENT:temporal_neuron]",
            "temporal": "[TYPE:temporal]",
            "nÃºcleo": "[SYSTEM:nucleus]",
            "entrenamiento": "[PROCESS:training]",
            "optimizaciÃ³n": "[IMPROVEMENT:optimization]",
            "consultar": "[ACTION:query]",
            "informaciÃ³n": "[REQUEST:information]"
        }
        
        for word in words:
            if word in metadata_keywords:
                tokens.append(metadata_keywords[word])
            elif domain.lower() in word:
                tokens.append(f"[DOMAIN:{domain}]")
            else:
                tokens.append(f"[WORD:{word}]")
        
        return tokens
    
    def _tokenize_metadata_output(self, response: str, domain: str) -> List[str]:
        """TokenizaciÃ³n de salida de metadatos"""
        tokens = []
        
        # Tokenizar con contexto de metadatos
        tokens.append(f"[DOMAIN_RESPONSE:{domain}]")
        
        for word in response.split()[:20]:  # Primeras 20 palabras
            if word.replace(".", "").isdigit() or "." in word:
                tokens.append(f"[METRIC:{word}]")
            elif word.lower() in ["precisiÃ³n", "experiencias", "optimizaciÃ³n"]:
                tokens.append(f"[KEY_CONCEPT:{word}]")
            else:
                tokens.append(f"[RESPONSE_WORD:{word}]")
        
        return tokens
    
    def _encode_metadata_int8(self, response: str, domain: str, precision: float) -> List[int]:
        """CodificaciÃ³n int8 de metadatos"""
        encoded = []
        
        # Bonus basado en precisiÃ³n y dominio
        precision_bonus = int(precision * 100)
        domain_bonus = {"ECU_ABS": 50, "Enhanced_Core": 40, "Hybrid_Fuzzy": 35, "Experience": 30}
        bonus = domain_bonus.get(domain.split("_")[0], 20) + precision_bonus
        
        for i, char in enumerate(response[:32]):
            base_value = ord(char) % 256
            
            # Modificar segÃºn contexto de metadatos
            if char.isdigit():
                base_value = (base_value + bonus + 30) % 256
            elif char in ['.', ',', ':']:
                base_value = (base_value + bonus + 20) % 256
            else:
                base_value = (base_value + bonus) % 256
            
            encoded.append(base_value)
        
        # Padding a 32 elementos
        while len(encoded) < 32:
            encoded.append(0)
        
        return encoded
    
    def _get_metadata_semantic_type(self, domain: str) -> str:
        """Tipo semÃ¡ntico de metadatos"""
        if "ECU" in domain:
            return "automotive_metadata"
        elif "Enhanced" in domain:
            return "optimization_metadata"
        elif "Hybrid" in domain:
            return "integration_metadata"
        elif "Experience" in domain:
            return "legacy_metadata"
        else:
            return "temporal_metadata"
    
    def _get_metadata_intent(self, input_text: str) -> str:
        """IntenciÃ³n de consulta de metadatos"""
        if "quÃ©" in input_text or "informaciÃ³n" in input_text:
            return "information_request"
        elif "experiencias" in input_text:
            return "experience_query"
        elif "optimizaciÃ³n" in input_text:
            return "optimization_query"
        elif "consultar" in input_text:
            return "database_query"
        else:
            return "metadata_request"
    
    def _get_metadata_complexity(self, metadata_info: Dict) -> str:
        """Complejidad de metadatos"""
        experiences = metadata_info["experiences"]
        if experiences > 100:
            return "expert"
        elif experiences > 30:
            return "advanced"
        elif experiences > 10:
            return "intermediate"
        else:
            return "beginner"
    
    def _get_metadata_aliases(self, domain: str) -> List[str]:
        """Aliases de dominio"""
        aliases = {
            "ECU_ABS_Diagnostics": ["ecu", "abs", "automotive", "diagnÃ³stico"],
            "Enhanced_Core_Optimization": ["enhanced", "optimized", "core", "optimizaciÃ³n"],
            "Hybrid_Fuzzy_Integration": ["hybrid", "fuzzy", "integration", "hÃ­brido"],
            "Experience_Database_Legacy": ["experience", "legacy", "database", "experiencia"]
        }
        return aliases.get(domain, [domain.lower()])
    
    def _create_metadata_fuzzy_map(self, response: str, domain: str) -> Dict:
        """Mapeo fuzzy de metadatos"""
        return {
            "exact_match": response,
            "domain_variants": [domain, domain.lower(), domain.replace("_", " ")],
            "concept_keywords": ["metadatos", "experiencias", "precisiÃ³n", "optimizaciÃ³n"],
            "similarity_threshold": 0.85
        }
    
    def _categorize_domain(self, domain: str) -> str:
        """Categorizar dominio"""
        if "ECU" in domain or "ABS" in domain:
            return "automotive"
        elif "Enhanced" in domain or "Optimization" in domain:
            return "system_optimization"
        elif "Hybrid" in domain or "Fuzzy" in domain:
            return "integration_systems"
        elif "Experience" in domain or "Database" in domain:
            return "knowledge_management"
        else:
            return "general_domain"
    
    def _get_precision_tier(self, precision: float) -> str:
        """Nivel de precisiÃ³n"""
        if precision >= 0.95:
            return "excellent"
        elif precision >= 0.8:
            return "high"
        elif precision >= 0.6:
            return "medium"
        else:
            return "developing"
    
    def _get_experience_level(self, experiences: int) -> str:
        """Nivel de experiencias"""
        if experiences >= 100:
            return "extensive"
        elif experiences >= 50:
            return "substantial"
        elif experiences >= 20:
            return "moderate"
        else:
            return "limited"
    
    def _calculate_metadata_complexity(self, metadata_info: Dict) -> int:
        """Calcular complejidad de metadatos"""
        score = 0
        score += min(metadata_info.get("experiences", 0) // 10, 5)
        score += min(metadata_info.get("metacompiler_patterns", 0) // 5, 3)
        if metadata_info.get("precision_achieved", 0) > 0.9:
            score += 2
        return min(score, 10)
    
    def _get_optimization_type(self, metadata_key: str) -> str:
        """Tipo de optimizaciÃ³n"""
        optimization_types = {
            "ecu_abs_metadata": "real_time_diagnostics",
            "enhanced_core_metadata": "function_pruning",
            "hybrid_fuzzy_metadata": "domain_integration",
            "experience_database_metadata": "knowledge_preservation"
        }
        return optimization_types.get(metadata_key, "general_optimization")
    
    def save_hybrid_dataset(self, dataset_pairs: List[Dict]) -> str:
        """Guardar dataset hÃ­brido"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"metadata_binaries_hybrid_dataset_{timestamp}.jsonl"
        filepath = self.dataset_dir / filename
        
        print("ğŸ’¾ Guardando dataset hÃ­brido metadatos + binarios...")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for pair in dataset_pairs:
                json_line = json.dumps(pair, ensure_ascii=False, separators=(',', ':'))
                f.write(json_line + '\n')
        
        file_size_mb = filepath.stat().st_size / 1024 / 1024
        
        print(f"âœ“ Dataset guardado: {filepath}")
        print(f"   â€¢ TamaÃ±o: {file_size_mb:.2f} MB")
        
        return str(filepath)
    
    def generate_complete_hybrid_dataset(self) -> Dict:
        """Generar dataset hÃ­brido completo"""
        print("\nğŸ”— GENERANDO DATASET HÃBRIDO METADATOS + BINARIOS")
        print("=" * 60)
        
        start_time = time.time()
        
        # Generar pares hÃ­bridos
        dataset_pairs = self.generate_hybrid_metadata_pairs()
        
        # Guardar dataset
        dataset_file = self.save_hybrid_dataset(dataset_pairs)
        
        generation_time = time.time() - start_time
        
        return {
            "dataset_file": dataset_file,
            "total_pairs": len(dataset_pairs),
            "generation_time": generation_time,
            "metadata_domains": len(self.unused_temporal_metadata),
            "hybrid_structure": True,
            "temporal_verified": True
        }


def main():
    """FunciÃ³n principal"""
    generator = MetadataBinariesHybridDataset()
    
    # Generar dataset hÃ­brido completo
    results = generator.generate_complete_hybrid_dataset()
    
    print(f"\nğŸ‰ Â¡DATASET HÃBRIDO METADATOS + BINARIOS COMPLETADO!")
    print(f"ğŸ”— Pares totales: {results['total_pairs']:,}")
    print(f"ğŸ“Š Dominios metadatos: {results['metadata_domains']}")
    print(f"â±ï¸ Tiempo: {results['generation_time']:.2f} segundos")
    print(f"ğŸ“ Archivo: {results['dataset_file']}")
    
    print(f"\nâœ… CARACTERÃSTICAS:")
    print(f"   âœ“ Metadatos temporales no utilizados")
    print(f"   âœ“ Binarios extraÃ­dos integrados")
    print(f"   âœ“ Estructura hÃ­brida semÃ¡ntico-binarizada")
    print(f"   âœ“ Formato compatible con entrenamientos")


if __name__ == "__main__":
    main()