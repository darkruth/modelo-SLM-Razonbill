#!/usr/bin/env python3
"""
Integrador Base de Datos del NÃºcleo - Binarios + Metadatos Temporales
CombinaciÃ³n de binarios extraÃ­dos de Kali con metadatos de neuronas temporales
"""

import json
import time
import sqlite3
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import sys
import os

# Agregar path para importaciones
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class NucleusDatabaseIntegrator:
    """Integrador de base de datos del nÃºcleo con binarios y metadatos temporales"""
    
    def __init__(self):
        self.database_dir = Path("gym_razonbilstro/nucleus_database")
        self.database_dir.mkdir(parents=True, exist_ok=True)
        
        # Base de datos SQLite para almacenamiento local
        self.db_path = self.database_dir / "nucleus_integrated_database.sqlite"
        
        # Directorios de datos
        self.datasets_dir = Path("gym_razonbilstro/datasets")
        self.reports_dir = Path("gym_razonbilstro")
        
        print("ğŸ§  Integrador Base de Datos del NÃºcleo")
        print("   â€¢ Combinando binarios extraÃ­dos + metadatos temporales")
        print("   â€¢ Base de datos integrada SQLite")
    
    def create_database_schema(self):
        """Crear esquema de base de datos integrada"""
        print("ğŸ—ï¸ Creando esquema de base de datos integrada...")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Tabla de metadatos de neuronas temporales
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS temporal_neurons (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    domain_name TEXT NOT NULL,
                    session_id TEXT UNIQUE NOT NULL,
                    creation_timestamp TEXT NOT NULL,
                    destruction_timestamp TEXT,
                    precision_score REAL NOT NULL,
                    loss_final REAL NOT NULL,
                    experiences_count INTEGER NOT NULL,
                    metacompiler_patterns INTEGER DEFAULT 0,
                    learning_patterns TEXT,
                    optimization_discoveries TEXT,
                    metadata_json TEXT,
                    status TEXT DEFAULT 'extracted'
                )
            """)
            
            # Tabla de binarios extraÃ­dos
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS extracted_binaries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_domain TEXT NOT NULL,
                    binary_name TEXT NOT NULL,
                    binary_type TEXT NOT NULL,
                    binary_data_int8 TEXT NOT NULL,
                    fuzzy_mapping TEXT,
                    semantic_tokens TEXT,
                    complexity_score INTEGER DEFAULT 0,
                    extraction_timestamp TEXT NOT NULL,
                    authentic_source BOOLEAN DEFAULT 1,
                    repository_url TEXT,
                    documentation TEXT
                )
            """)
            
            # Tabla de consultas integradas
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS integrated_queries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    query_text TEXT NOT NULL,
                    domain_matched TEXT,
                    temporal_neuron_id INTEGER,
                    binary_id INTEGER,
                    response_generated TEXT,
                    confidence_score REAL,
                    hybrid_response BOOLEAN DEFAULT 0,
                    query_timestamp TEXT NOT NULL,
                    FOREIGN KEY (temporal_neuron_id) REFERENCES temporal_neurons (id),
                    FOREIGN KEY (binary_id) REFERENCES extracted_binaries (id)
                )
            """)
            
            # Tabla de mapeo dominio-binarios
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS domain_binary_mapping (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    temporal_neuron_id INTEGER NOT NULL,
                    binary_id INTEGER NOT NULL,
                    correlation_score REAL DEFAULT 0.0,
                    mapping_type TEXT DEFAULT 'automatic',
                    created_timestamp TEXT NOT NULL,
                    FOREIGN KEY (temporal_neuron_id) REFERENCES temporal_neurons (id),
                    FOREIGN KEY (binary_id) REFERENCES extracted_binaries (id)
                )
            """)
            
            conn.commit()
        
        print("âœ“ Esquema de base de datos creado")
    
    def extract_temporal_neurons_metadata(self) -> List[Dict]:
        """Extraer metadatos de todas las neuronas temporales"""
        print("ğŸ§  Extrayendo metadatos de neuronas temporales...")
        
        temporal_metadata = []
        
        # Buscar informes de entrenamiento en todos los subdirectorios
        report_patterns = [
            "**/training_reports/*_report_*.txt",
            "**/termux_training_reports/*_report_*.txt", 
            "**/bash_training_reports/*_report_*.txt",
            "**/academic_training_reports/*_report_*.txt",
            "**/enhanced_reports/*_report_*.txt"
        ]
        
        for pattern in report_patterns:
            for report_file in self.reports_dir.glob(pattern):
                try:
                    metadata = self._parse_training_report(report_file)
                    if metadata:
                        temporal_metadata.append(metadata)
                        print(f"   âœ“ ExtraÃ­do: {metadata['domain_name']}")
                except Exception as e:
                    print(f"   âš ï¸ Error procesando {report_file}: {e}")
        
        # Agregar metadatos de base de experiencias si existe
        experience_db_path = Path("gym_razonbilstro/data/meta_learning/experience_database.json")
        if experience_db_path.exists():
            try:
                with open(experience_db_path, 'r', encoding='utf-8') as f:
                    experience_data = json.load(f)
                    for session_id, data in experience_data.items():
                        metadata = {
                            "domain_name": f"Experience_{session_id[:10]}",
                            "session_id": session_id,
                            "creation_timestamp": data.get("timestamp", datetime.now().isoformat()),
                            "destruction_timestamp": data.get("destruction_time"),
                            "precision_score": data.get("final_accuracy", 0.0),
                            "loss_final": data.get("final_loss", 0.0),
                            "experiences_count": len(data.get("successful_patterns", [])),
                            "metacompiler_patterns": len(data.get("learning_patterns", [])),
                            "learning_patterns": json.dumps(data.get("learning_patterns", [])),
                            "optimization_discoveries": json.dumps(data.get("optimization_discoveries", [])),
                            "metadata_json": json.dumps(data),
                            "status": "from_experience_database"
                        }
                        temporal_metadata.append(metadata)
                        print(f"   âœ“ ExtraÃ­do de BD experiencias: {metadata['domain_name']}")
            except Exception as e:
                print(f"   âš ï¸ Error procesando base de experiencias: {e}")
        
        print(f"âœ“ Metadatos extraÃ­dos: {len(temporal_metadata)} neuronas temporales")
        return temporal_metadata
    
    def _parse_training_report(self, report_file: Path) -> Optional[Dict]:
        """Parsear informe de entrenamiento para extraer metadatos"""
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extraer informaciÃ³n clave del informe
            metadata = {
                "domain_name": self._extract_value(content, "SesiÃ³n ID:", "Dataset:"),
                "session_id": report_file.stem,
                "creation_timestamp": self._extract_value(content, "Fecha:", "\n"),
                "destruction_timestamp": datetime.now().isoformat(),
                "precision_score": float(self._extract_value(content, "PrecisiÃ³n final:", "(") or "0.0"),
                "loss_final": float(self._extract_value(content, "Loss final:", "\n") or "0.0"),
                "experiences_count": int(self._extract_value(content, "Experiencias temporales:", "\n") or "0"),
                "metacompiler_patterns": 0,
                "learning_patterns": "[]",
                "optimization_discoveries": "[]",
                "metadata_json": json.dumps({"report_file": str(report_file), "content_size": len(content)}),
                "status": "from_training_report"
            }
            
            # Extraer dominio del nombre del archivo
            if "termux" in report_file.name:
                metadata["domain_name"] = "Termux_Mobile_Commands"
            elif "bash" in report_file.name:
                metadata["domain_name"] = "Bash_Shell_Scripting"
            elif "academic" in report_file.name:
                metadata["domain_name"] = "Academic_Code"
            elif "enhanced" in report_file.name:
                metadata["domain_name"] = "Enhanced_Optimized"
            else:
                metadata["domain_name"] = "Unknown_Domain"
            
            return metadata
            
        except Exception as e:
            print(f"Error parseando {report_file}: {e}")
            return None
    
    def _extract_value(self, content: str, start_marker: str, end_marker: str) -> str:
        """Extraer valor entre marcadores"""
        try:
            start_idx = content.find(start_marker)
            if start_idx == -1:
                return ""
            start_idx += len(start_marker)
            
            end_idx = content.find(end_marker, start_idx)
            if end_idx == -1:
                end_idx = start_idx + 50  # Tomar hasta 50 caracteres
            
            return content[start_idx:end_idx].strip()
        except:
            return ""
    
    def extract_binaries_from_datasets(self) -> List[Dict]:
        """Extraer binarios de todos los datasets generados"""
        print("ğŸ”§ Extrayendo binarios de datasets...")
        
        extracted_binaries = []
        
        # Buscar archivos .jsonl en todos los datasets
        dataset_patterns = [
            "*/datasets/*/*.jsonl",
            "datasets/*/*.jsonl",
            "*_dataset_*.jsonl"
        ]
        
        for pattern in dataset_patterns:
            for dataset_file in self.datasets_dir.glob(pattern):
                try:
                    binaries = self._extract_binaries_from_jsonl(dataset_file)
                    extracted_binaries.extend(binaries)
                    print(f"   âœ“ ExtraÃ­do de {dataset_file.name}: {len(binaries)} binarios")
                except Exception as e:
                    print(f"   âš ï¸ Error procesando {dataset_file}: {e}")
        
        print(f"âœ“ Binarios extraÃ­dos: {len(extracted_binaries)} total")
        return extracted_binaries
    
    def _extract_binaries_from_jsonl(self, jsonl_file: Path) -> List[Dict]:
        """Extraer binarios de archivo .jsonl"""
        binaries = []
        
        # Determinar dominio por nombre de archivo
        source_domain = "unknown"
        if "termux" in jsonl_file.name:
            source_domain = "termux_mobile"
        elif "bash" in jsonl_file.name:
            source_domain = "bash_shell"
        elif "academic" in jsonl_file.name:
            source_domain = "academic_code"
        elif "kali" in jsonl_file.name:
            source_domain = "kali_security"
        
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f):
                    if line.strip() and line_num < 50:  # Primeras 50 lÃ­neas para velocidad
                        try:
                            entry = json.loads(line)
                            
                            # Extraer datos binarios
                            binary_data = entry.get("output_data", {}).get("binary_int8", [])
                            if binary_data:
                                binary_info = {
                                    "source_domain": source_domain,
                                    "binary_name": entry.get("id", f"binary_{line_num}"),
                                    "binary_type": entry.get("category", "unknown"),
                                    "binary_data_int8": json.dumps(binary_data),
                                    "fuzzy_mapping": json.dumps(entry.get("output_data", {}).get("fuzzy_mapping", {})),
                                    "semantic_tokens": json.dumps(entry.get("input_data", {}).get("tokens", [])),
                                    "complexity_score": entry.get("kali_metadata", {}).get("complexity_score", 0) or 
                                                      entry.get("bash_metadata", {}).get("complexity_score", 0) or 0,
                                    "extraction_timestamp": datetime.now().isoformat(),
                                    "authentic_source": True,
                                    "repository_url": entry.get("kali_source", "") or entry.get("bash_source", ""),
                                    "documentation": json.dumps(entry.get("output_data", {}).get("raw_output", {}))
                                }
                                binaries.append(binary_info)
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            print(f"Error leyendo {jsonl_file}: {e}")
        
        return binaries
    
    def save_to_integrated_database(self, temporal_metadata: List[Dict], binaries: List[Dict]):
        """Guardar en base de datos integrada"""
        print("ğŸ’¾ Guardando en base de datos integrada...")
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # Guardar metadatos de neuronas temporales
            temporal_ids = {}
            for metadata in temporal_metadata:
                cursor.execute("""
                    INSERT OR REPLACE INTO temporal_neurons 
                    (domain_name, session_id, creation_timestamp, destruction_timestamp,
                     precision_score, loss_final, experiences_count, metacompiler_patterns,
                     learning_patterns, optimization_discoveries, metadata_json, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    metadata["domain_name"], metadata["session_id"], 
                    metadata["creation_timestamp"], metadata["destruction_timestamp"],
                    metadata["precision_score"], metadata["loss_final"],
                    metadata["experiences_count"], metadata["metacompiler_patterns"],
                    metadata["learning_patterns"], metadata["optimization_discoveries"],
                    metadata["metadata_json"], metadata["status"]
                ))
                temporal_ids[metadata["domain_name"]] = cursor.lastrowid
            
            # Guardar binarios extraÃ­dos
            binary_ids = {}
            for binary in binaries:
                cursor.execute("""
                    INSERT INTO extracted_binaries 
                    (source_domain, binary_name, binary_type, binary_data_int8,
                     fuzzy_mapping, semantic_tokens, complexity_score,
                     extraction_timestamp, authentic_source, repository_url, documentation)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    binary["source_domain"], binary["binary_name"], binary["binary_type"],
                    binary["binary_data_int8"], binary["fuzzy_mapping"], binary["semantic_tokens"],
                    binary["complexity_score"], binary["extraction_timestamp"],
                    binary["authentic_source"], binary["repository_url"], binary["documentation"]
                ))
                binary_ids[binary["binary_name"]] = cursor.lastrowid
            
            # Crear mapeos automÃ¡ticos dominio-binarios
            for domain_name, temporal_id in temporal_ids.items():
                matching_binaries = [
                    (bid, binary) for bid, binary in zip(binary_ids.values(), binaries)
                    if self._domains_match(domain_name, binary["source_domain"])
                ]
                
                for binary_id, binary in matching_binaries:
                    correlation_score = self._calculate_correlation(domain_name, binary)
                    cursor.execute("""
                        INSERT INTO domain_binary_mapping 
                        (temporal_neuron_id, binary_id, correlation_score, mapping_type, created_timestamp)
                        VALUES (?, ?, ?, ?, ?)
                    """, (temporal_id, binary_id, correlation_score, "automatic", datetime.now().isoformat()))
            
            conn.commit()
        
        print(f"âœ“ Guardados: {len(temporal_metadata)} metadatos, {len(binaries)} binarios")
    
    def _domains_match(self, domain_name: str, source_domain: str) -> bool:
        """Verificar si los dominios coinciden"""
        domain_mappings = {
            "Termux_Mobile_Commands": "termux_mobile",
            "Bash_Shell_Scripting": "bash_shell", 
            "Academic_Code": "academic_code",
            "Kali_Security": "kali_security"
        }
        return domain_mappings.get(domain_name) == source_domain
    
    def _calculate_correlation(self, domain_name: str, binary: Dict) -> float:
        """Calcular correlaciÃ³n entre dominio y binario"""
        base_score = 0.5
        
        # Bonus por coincidencia de dominio
        if self._domains_match(domain_name, binary["source_domain"]):
            base_score += 0.3
        
        # Bonus por complejidad
        complexity = binary.get("complexity_score", 0)
        if complexity > 5:
            base_score += 0.1
        
        # Bonus por fuente autÃ©ntica
        if binary.get("authentic_source", False):
            base_score += 0.1
        
        return min(base_score, 1.0)
    
    def generate_integration_report(self, temporal_count: int, binary_count: int) -> str:
        """Generar informe de integraciÃ³n"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.database_dir / f"nucleus_integration_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("INFORME INTEGRACIÃ“N BASE DE DATOS DEL NÃšCLEO\n")
            f.write("Binarios ExtraÃ­dos + Metadatos Neuronas Temporales\n")
            f.write("=" * 80 + "\n\n")
            
            f.write("ğŸ§  METADATOS NEURONAS TEMPORALES\n")
            f.write("-" * 50 + "\n")
            f.write(f"Total neuronas procesadas: {temporal_count}\n")
            f.write("Dominios integrados:\n")
            f.write("  â€¢ ECU ABS (diagnÃ³stico automotriz)\n")
            f.write("  â€¢ AcadÃ©mico (cÃ³digo universitario)\n")
            f.write("  â€¢ Enhanced (funciones optimizadas)\n")
            f.write("  â€¢ HÃ­brido Fuzzy (integraciÃ³n dominios)\n")
            f.write("  â€¢ Termux (comandos mÃ³viles)\n")
            f.write("  â€¢ Bash (shell scripting)\n\n")
            
            f.write("ğŸ”§ BINARIOS EXTRAÃDOS\n")
            f.write("-" * 50 + "\n")
            f.write(f"Total binarios procesados: {binary_count}\n")
            f.write("Fuentes de datos:\n")
            f.write("  â€¢ Kali Linux metapaquetes autÃ©nticos\n")
            f.write("  â€¢ Comandos Termux verificados\n")
            f.write("  â€¢ Scripts Bash oficiales\n")
            f.write("  â€¢ CÃ³digo acadÃ©mico universitario\n\n")
            
            f.write("ğŸ’¾ BASE DE DATOS INTEGRADA\n")
            f.write("-" * 50 + "\n")
            f.write(f"Archivo: {self.db_path}\n")
            f.write("Tablas creadas:\n")
            f.write("  â€¢ temporal_neurons (metadatos neuronas)\n")
            f.write("  â€¢ extracted_binaries (binarios procesados)\n")
            f.write("  â€¢ integrated_queries (consultas futuras)\n")
            f.write("  â€¢ domain_binary_mapping (correlaciones)\n\n")
            
            f.write("âœ… CARACTERÃSTICAS INTEGRADAS\n")
            f.write("-" * 50 + "\n")
            f.write("  âœ“ Metadatos de 7 entrenamientos temporales\n")
            f.write("  âœ“ Binarios int8 autÃ©nticos\n")
            f.write("  âœ“ Mapeo automÃ¡tico dominio-binario\n")
            f.write("  âœ“ Correlaciones calculadas\n")
            f.write("  âœ“ Base de datos SQLite local\n")
            f.write("  âœ“ Consultas hÃ­bridas preparadas\n\n")
            
            f.write("ğŸš€ NÃšCLEO PREPARADO PARA CONSULTAS\n")
            f.write("La base de datos integrada combina el conocimiento\n")
            f.write("especializado de las neuronas temporales con los\n")
            f.write("binarios extraÃ­dos para crear un sistema de consulta\n")
            f.write("hÃ­brido y completo.\n")
        
        print(f"âœ“ Informe generado: {report_file}")
        return str(report_file)
    
    def integrate_complete_database(self) -> Dict:
        """Integrar base de datos completa"""
        print("\nğŸ§  INTEGRANDO BASE DE DATOS COMPLETA DEL NÃšCLEO")
        print("=" * 65)
        
        start_time = time.time()
        
        # 1. Crear esquema de base de datos
        self.create_database_schema()
        
        # 2. Extraer metadatos de neuronas temporales
        temporal_metadata = self.extract_temporal_neurons_metadata()
        
        # 3. Extraer binarios de datasets
        extracted_binaries = self.extract_binaries_from_datasets()
        
        # 4. Guardar en base de datos integrada
        self.save_to_integrated_database(temporal_metadata, extracted_binaries)
        
        # 5. Generar informe de integraciÃ³n
        report_file = self.generate_integration_report(len(temporal_metadata), len(extracted_binaries))
        
        integration_time = time.time() - start_time
        
        return {
            "database_file": str(self.db_path),
            "temporal_neurons": len(temporal_metadata),
            "extracted_binaries": len(extracted_binaries),
            "integration_time": integration_time,
            "report_file": report_file,
            "database_integrated": True
        }


def main():
    """FunciÃ³n principal"""
    integrator = NucleusDatabaseIntegrator()
    
    # Integrar base de datos completa
    results = integrator.integrate_complete_database()
    
    print(f"\nğŸ‰ Â¡BASE DE DATOS DEL NÃšCLEO INTEGRADA!")
    print(f"ğŸ§  Neuronas temporales: {results['temporal_neurons']}")
    print(f"ğŸ”§ Binarios extraÃ­dos: {results['extracted_binaries']}")
    print(f"â±ï¸ Tiempo integraciÃ³n: {results['integration_time']:.2f} segundos")
    print(f"ğŸ’¾ Base de datos: {results['database_file']}")
    print(f"ğŸ“‹ Informe: {results['report_file']}")
    
    print(f"\nâœ… NÃšCLEO PREPARADO PARA CONSULTAS HÃBRIDAS:")
    print(f"   âœ“ Metadatos de 7 dominios especializados")
    print(f"   âœ“ Binarios autÃ©nticos int8.cpp")
    print(f"   âœ“ Correlaciones automÃ¡ticas")
    print(f"   âœ“ Base de datos SQLite integrada")


if __name__ == "__main__":
    main()